{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2209261 Basic Programming NLP\n",
    "## Lab 12 : NLP (Part II Large Language Model)\n",
    "## Done by : 6730084521 Chatrphol Ovanonchai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cT7y--9LUd8"
   },
   "source": [
    "# 1. Generate a Jokes Dataset with an LLM\n",
    "\n",
    "Write a Python script that calls the Gemini API to generate a small jokes dataset based on a user-provided topic and number of jokes, then returns the results as a pandas DataFrame.\n",
    "\n",
    "## Functional Requirements\n",
    "1. **Inputs**\n",
    "   - `topic` (string), e.g., `\"computers\"`, `\"Thai food\"`.\n",
    "   - `n_jokes` (int), e.g., `20`.\n",
    "\n",
    "2. **LLM Call (Gemini API)**\n",
    "   - Prompt Gemini to generate `n_jokes` short, family-friendly jokes about the given topic.\n",
    "   - Ask Gemini to return the output in **strict JSON** format with the following structure.\n",
    "\n",
    "3. **Expected Output Schema**\n",
    "   | Column | Type | Description |\n",
    "   |---------|------|--------------|\n",
    "   | id | int | Index number (1..n) |\n",
    "   | topic | str | The topic used |\n",
    "   | joke | str | The joke text |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries\n",
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 761
    },
    "executionInfo": {
     "elapsed": 7041,
     "status": "ok",
     "timestamp": 1761649095766,
     "user": {
      "displayName": "jakapun ta",
      "userId": "02776561496860281329"
     },
     "user_tz": -420
    },
    "id": "aadbd38d",
    "outputId": "81f75b99-99df-4ed2-f2b7-f8b80752baad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's imagine you want to teach a computer to distinguish between pictures of cats and dogs.\n",
      "\n",
      "**Generative Model: The \"Artist\"**\n",
      "\n",
      "Think of a generative model as an **artist** who learns how to **create** new pictures of cats and dogs.\n",
      "\n",
      "*   **What it learns:** It tries to understand *what makes a cat a cat* and *what makes a dog a dog*. It learns the typical features of each, like the shape of their ears, eyes, nose, fur patterns, and body structure. It's essentially learning the **underlying distribution** of data for each category.\n",
      "*   **How it works:** It learns the probability of seeing certain pixels or features *given* that it's a cat, and the probability of seeing certain pixels or features *given* that it's a dog.\n",
      "*   **What it can do:**\n",
      "    *   **Generate new examples:** It can create entirely new, realistic-looking pictures of cats and dogs that it has never seen before. It can \"imagine\" what a new cat or dog would look like.\n",
      "    *   **Classify:** Since it understands what cats and dogs look like, it can also look at a new picture and decide if it's more likely to be a cat or a dog.\n",
      "*   **Analogy:** Imagine learning to draw. You learn the proportions, the curves, the textures that define a cat. Then, you can draw a new cat from scratch.\n",
      "\n",
      "**Discriminative Model: The \"Judge\"**\n",
      "\n",
      "Think of a discriminative model as a **judge** who learns to **distinguish** between existing pictures of cats and dogs.\n",
      "\n",
      "*   **What it learns:** It focuses on finding the **boundaries** or **differences** between cats and dogs. It learns what features are most important for telling them apart. It doesn't necessarily care about *creating* a cat, just about *identifying* one.\n",
      "*   **How it works:** It directly learns the probability of a label (cat or dog) *given* the input image. It's trying to learn a function that maps an image to a category.\n",
      "*   **What it can do:**\n",
      "    *   **Classify:** Its primary job is to look at a new picture and tell you, \"This is a cat\" or \"This is a dog.\"\n",
      "    *   **It cannot generate new examples:** It doesn't learn enough about the \"essence\" of a cat to draw a new one.\n",
      "*   **Analogy:** Imagine you're shown many pictures and told which ones are cats and which are dogs. You learn to spot the key differences: \"Cats have pointy ears and slit pupils, while dogs have floppier ears and rounder pupils.\" You become good at telling them apart.\n",
      "\n",
      "**Here's a table to summarize:**\n",
      "\n",
      "| Feature         | Generative Model                      | Discriminative Model                  |\n",
      "| :-------------- | :------------------------------------ | :------------------------------------ |\n",
      "| **Goal**        | Learn to **create** data              | Learn to **distinguish** between data |\n",
      "| **Learns**      | How data is generated for each class  | The boundary between classes          |\n",
      "| **Key Question** | \"What does a cat/dog *look like*?\"    | \"Is this a cat *or* a dog*?\"          |\n",
      "| **Capabilities** | Generate new data, Classify           | Classify                              |\n",
      "| **Analogy**     | Artist                                | Judge                                 |\n",
      "\n",
      "**In Simple Terms:**\n",
      "\n",
      "*   **Generative models** try to understand the **whole story** of each category so they can tell new stories (generate new examples) and also identify them.\n",
      "*   **Discriminative models** focus on learning just enough **differences** to tell things apart effectively.\n",
      "\n",
      "**Which is better?**\n",
      "\n",
      "It depends on the task!\n",
      "\n",
      "*   If you want to create new images, music, or text, you need a **generative model**.\n",
      "*   If your main goal is to accurately classify existing data (like spam filters, image recognition), a **discriminative model** is often more efficient and can achieve higher accuracy for that specific task.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "api_key = \"AIzaSyBoZ3ltxyq3IZKksNGB19SvNcH6S2nPRyQ\"\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n",
    "\n",
    "prompt = \"Explain the difference between a Generative model and a Discriminative model in simple terms.\"\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Department of Electrical Engineering at Chulalongkorn University boasts a rich and foundational history, deeply intertwined with the development of electrical engineering education and practice in Thailand. Here's a breakdown of its evolution:\n",
      "\n",
      "**Early Beginnings and the Genesis of Engineering Education (Pre-1940s):**\n",
      "\n",
      "* **The Seeds of Engineering:** While not a dedicated Electrical Engineering department initially, the concept of engineering education at Chulalongkorn University began to take root in the early 20th century. The university was established in 1917, and the need for skilled professionals in various fields, including technical ones, was recognized.\n",
      "* **The \"School of Engineering\" (1930s):** The formal establishment of an engineering faculty, or \"School of Engineering,\" was a crucial step. This broad faculty encompassed various engineering disciplines, laying the groundwork for specialization.\n",
      "\n",
      "**The Birth of Electrical Engineering as a Distinct Discipline (1940s):**\n",
      "\n",
      "* **Formal Establishment of the Department:** The **Department of Electrical Engineering** as a standalone entity was officially established in **1940**. This marked a significant milestone, recognizing the growing importance and complexity of electrical engineering as a field of study.\n",
      "* **Early Focus:** In its nascent stages, the curriculum likely focused on fundamental principles of electricity, power generation and distribution, basic electronics, and communication systems. The aim was to produce graduates capable of contributing to the nation's nascent industrial\n"
     ]
    }
   ],
   "source": [
    "## Optional : Tuning output (TEst with prompt)\n",
    "prompt = \"Explain history of department of Elecrial Engineering , Chulalongkorn University \"\n",
    "\n",
    "# Define the generation configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 0.7,  # A higher temperature for creativity\n",
    "    \"max_output_tokens\": 300 # Keep the story relatively short\n",
    "}\n",
    "\n",
    "# Make the API call with the configuration\n",
    "try:\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c36418c1"
   },
   "source": [
    "## Define function for joke generation\n",
    "\n",
    "### Subtask:\n",
    "Create a Python function that takes the topic and number of jokes as input, calls the Gemini API to generate jokes in JSON format, and returns the JSON response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18e7a607"
   },
   "source": [
    "**Reasoning**:\n",
    "Define the Python function to generate jokes using the Gemini API, construct the prompt, call the API, extract the JSON response, and return it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1761645313523,
     "user": {
      "displayName": "jakapun ta",
      "userId": "02776561496860281329"
     },
     "user_tz": -420
    },
    "id": "ef7fe529"
   },
   "outputs": [],
   "source": [
    "# import json parse libraries\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "def generate_jokes_from_gemini(topic: str, n_jokes: int):\n",
    "\n",
    "    # Step 1 : create prompt f-string \n",
    "    prompt = f\"Generate jokes about {topic} with quantity of {n_jokes} jokes , need it in JSON format having id , topics (value = {topic}) and joke keys\"\n",
    "    \n",
    "    # Step 2 : Tuning output with generation_config\n",
    "    generation_config = {\n",
    "        \"temperature\": 0.72,  # A higher temperature for creativity\n",
    "        \"max_output_tokens\": 500 , # Keep the story relatively short\n",
    "        \"response_mime_type\": \"application/json\" # return JSON Type\n",
    "    }\n",
    "    \n",
    "    # Step 3 : generate content with tuned output\n",
    "    response = model.generate_content(prompt , generation_config = generation_config)\n",
    "    \n",
    "    # Step 4 : return as response.text (JSON Format)\n",
    "    data = response.text\n",
    "    return data # JSON String\n",
    "\n",
    "def parse_jokes_json_to_dataframe(jokes_json):\n",
    "    \n",
    "    # Step 1 : loads JSON String to list/dict\n",
    "    jokes_data = json.loads(jokes_json)         # string ‚Üí Python list/dict\n",
    "    \n",
    "    # Step 2 ; using json_normalize to convert string to dataframe\n",
    "    output_df = json_normalize(jokes_data, sep=\"_\")    # flatten JSON\n",
    "    \n",
    "    # Step 3 : return dataframe\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "executionInfo": {
     "elapsed": 2603,
     "status": "ok",
     "timestamp": 1761645320721,
     "user": {
      "displayName": "jakapun ta",
      "userId": "02776561496860281329"
     },
     "user_tz": -420
    },
    "id": "2130ba07",
    "outputId": "d6a230d3-a8f6-4d69-b922-9b7725e97e4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"topics\": \"animals\",\n",
      "    \"joke\": \"Why don't scientists trust atoms? Because they make up everything! (Just kidding, that's a science joke, here's an animal one: What do you call a lazy kangaroo? Pouch potato!)\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"topics\": \"animals\",\n",
      "    \"joke\": \"What do you call a bear with no teeth? A gummy bear!\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"topics\": \"animals\",\n",
      "    \"joke\": \"Why did the scarecrow win an award? Because he was outstanding in his field! (Okay, another animal one: What do you get when you cross a snowman and a vampire? Frostbite!)\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 4,\n",
      "    \"topics\": \"animals\",\n",
      "    \"joke\": \"What do you call a fish with no eyes? Fsh!\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 5,\n",
      "    \"topics\": \"animals\",\n",
      "    \"joke\": \"Why was the elephant kicked out of the swimming pool? He kept dropping his trunks!\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topics</th>\n",
       "      <th>joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>animals</td>\n",
       "      <td>Why don't scientists trust atoms? Because they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>animals</td>\n",
       "      <td>What do you call a bear with no teeth? A gummy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>animals</td>\n",
       "      <td>Why did the scarecrow win an award? Because he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>animals</td>\n",
       "      <td>What do you call a fish with no eyes? Fsh!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>animals</td>\n",
       "      <td>Why was the elephant kicked out of the swimmin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   topics                                               joke\n",
       "0   1  animals  Why don't scientists trust atoms? Because they...\n",
       "1   2  animals  What do you call a bear with no teeth? A gummy...\n",
       "2   3  animals  Why did the scarecrow win an award? Because he...\n",
       "3   4  animals         What do you call a fish with no eyes? Fsh!\n",
       "4   5  animals  Why was the elephant kicked out of the swimmin..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic = \"animals\"\n",
    "n_jokes = 5\n",
    "\n",
    "jokes_json = generate_jokes_from_gemini(topic, n_jokes)\n",
    "print(jokes_json)\n",
    "\n",
    "jokes_df = parse_jokes_json_to_dataframe(jokes_json)\n",
    "\n",
    "display(jokes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 1821,
     "status": "ok",
     "timestamp": 1761645322549,
     "user": {
      "displayName": "jakapun ta",
      "userId": "02776561496860281329"
     },
     "user_tz": -420
    },
    "id": "bf13287e",
    "outputId": "fb11121b-5e9a-4243-88f6-a3ecc2a8e8a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topics</th>\n",
       "      <th>joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>science</td>\n",
       "      <td>Why did the physicist break up with the biolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>science</td>\n",
       "      <td>What do you call a lazy kangaroo? Pouch potato!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>science</td>\n",
       "      <td>Why don't scientists trust atoms? Because they...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   topics                                               joke\n",
       "0   1  science  Why did the physicist break up with the biolog...\n",
       "1   2  science    What do you call a lazy kangaroo? Pouch potato!\n",
       "2   3  science  Why don't scientists trust atoms? Because they..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic = \"science\"\n",
    "n_jokes = 3\n",
    "\n",
    "jokes_json = generate_jokes_from_gemini(topic, n_jokes)\n",
    "jokes_df = parse_jokes_json_to_dataframe(jokes_json)\n",
    "\n",
    "display(jokes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2rUV7e1VR7B"
   },
   "source": [
    "# Task 2 ‚Äî Classify Sentiment with Gemini (LLM)\n",
    "\n",
    "##  Goal\n",
    "Use the **Gemini API** to classify the sentiment of social-media messages from the **`wisesight_sentiment`** dataset, then evaluate model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 2.1 ‚Äî Sentiment Classification with LLM\n",
    "1. Call the **Gemini API** to classify sentiment for each message in the dataset.\n",
    "2. **Batch processing recommended**:  \n",
    "   - Send messages in batches (e.g., **20 rows at a time**) to reduce overhead.\n",
    "3. The model‚Äôs **output must contain only the sentiment label**  \n",
    "   - No explanation, reasoning, or extra text.\n",
    "   - Example expected outputs:  \n",
    "     - `\"positive\"`  \n",
    "     - `\"neutral\"`  \n",
    "     - `\"negative\"`\n",
    "4. Store the predicted sentiment label in a **new column** in your DataFrame (e.g., `pred_sentiment`).\n",
    "\n",
    "---\n",
    "\n",
    "## Task 2.2 ‚Äî Evaluate Performance\n",
    "Write a function to compute the following metrics:\n",
    "\n",
    "### **Accuracy**\n",
    "Measures the proportion of correctly predicted labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>category</th>\n",
       "      <th>pred_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡∏Å‡∏π‡∏à‡∏∞‡πÑ‡∏õ‡∏î‡∏π‡∏î‡πÉ‡∏ô‡πÄ‡∏£‡∏∑‡∏≠‡∏î‡∏≥‡∏ô‡πâ‡∏≥‡∏Ç‡∏≠‡∏á‡∏ô‡∏≤‡∏¢‡∏Å üòÖ</td>\n",
       "      <td>neu</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°!! ‡∏ù‡∏∂‡∏Å‡∏£‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏•‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö ‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏≤‡∏û‡∏ö...</td>\n",
       "      <td>neu</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡∏≠‡∏∏‡∏ï‡∏£‡∏î‡∏¥‡∏ï‡∏ñ‡πå‡∏°‡∏µ‡∏´‡∏°‡πâ‡∏≠‡∏ô‡πâ‡∏≥‡∏ã‡∏∏‡∏õ4‡∏ä‡πà‡∏≠‡∏á‡πÑ‡∏´‡∏°‡∏Ñ‡πà‡∏∞</td>\n",
       "      <td>neu</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ShowDC ‡∏á‡∏≤‡∏ô‡∏î‡∏µ ‡πÄ‡∏£‡∏≤‡πÄ‡∏Ñ‡∏¢‡∏¢‡∏¢üëçüëçüëç</td>\n",
       "      <td>pos</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡∏õ‡∏õ‡∏õ‡∏õ.))</td>\n",
       "      <td>neu</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>‡∏Ç‡∏ô‡πÄ‡∏™‡∏î‡πÑ‡∏á</td>\n",
       "      <td>neu</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∏‡πà‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö Honda Civic ‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏ñ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏ß‡∏•‡∏≤</td>\n",
       "      <td>neg</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>‡∏™‡∏á‡∏™‡∏≤‡∏£‡πÑ‡∏≠‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å‡∏≠‡∏∞‡∏î‡∏¥</td>\n",
       "      <td>neu</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡πÄ‡∏ö‡∏•‡πá‡∏Å ‡∏£‡∏™‡∏ù‡∏≤‡∏î‡πÄ‡∏ã‡∏•‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏Ç‡∏ß‡∏î‡∏ô‡∏∂‡∏á</td>\n",
       "      <td>neu</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                texts category pred_sentiment\n",
       "0                       ‡∏Å‡∏π‡∏à‡∏∞‡πÑ‡∏õ‡∏î‡∏π‡∏î‡πÉ‡∏ô‡πÄ‡∏£‡∏∑‡∏≠‡∏î‡∏≥‡∏ô‡πâ‡∏≥‡∏Ç‡∏≠‡∏á‡∏ô‡∏≤‡∏¢‡∏Å üòÖ      neu           None\n",
       "1   ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°!! ‡∏ù‡∏∂‡∏Å‡∏£‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏•‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö ‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏≤‡∏û‡∏ö...      neu           None\n",
       "2                    ‡∏≠‡∏∏‡∏ï‡∏£‡∏î‡∏¥‡∏ï‡∏ñ‡πå‡∏°‡∏µ‡∏´‡∏°‡πâ‡∏≠‡∏ô‡πâ‡∏≥‡∏ã‡∏∏‡∏õ4‡∏ä‡πà‡∏≠‡∏á‡πÑ‡∏´‡∏°‡∏Ñ‡πà‡∏∞      neu           None\n",
       "3                            ShowDC ‡∏á‡∏≤‡∏ô‡∏î‡∏µ ‡πÄ‡∏£‡∏≤‡πÄ‡∏Ñ‡∏¢‡∏¢‡∏¢üëçüëçüëç      pos           None\n",
       "4                                             ‡∏õ‡∏õ‡∏õ‡∏õ.))      neu           None\n",
       "..                                                ...      ...            ...\n",
       "95                                            ‡∏Ç‡∏ô‡πÄ‡∏™‡∏î‡πÑ‡∏á      neu           None\n",
       "96  ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∏‡πà‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö Honda Civic ‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏ñ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ...      pos           None\n",
       "97                                           ‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏ß‡∏•‡∏≤      neg           None\n",
       "98                                 ‡∏™‡∏á‡∏™‡∏≤‡∏£‡πÑ‡∏≠‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å‡∏≠‡∏∞‡∏î‡∏¥      neu           None\n",
       "99                ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡πÄ‡∏ö‡∏•‡πá‡∏Å ‡∏£‡∏™‡∏ù‡∏≤‡∏î‡πÄ‡∏ã‡∏•‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏Ç‡∏ß‡∏î‡∏ô‡∏∂‡∏á      neu           None\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "def classify_sentiment_batch(texts):\n",
    "    \"\"\"\n",
    "    Send a batch of texts to Gemini and return a list of sentiment labels.\n",
    "    Output must be ONLY: neg / neu / pos\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Classify sentiment for each message.\n",
    "Return ONLY one label per line using: pos, neu, neg.\n",
    "NO explanation.\n",
    "\n",
    "Messages:\n",
    "{texts}\n",
    "\"\"\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    labels = response.text.strip().split(\"\\n\")\n",
    "    labels = [x.strip().lower() for x in labels]\n",
    "\n",
    "    return labels\n",
    "\n",
    "batch_size = 20\n",
    "preds = []\n",
    "texts = df[\"texts\"].astype(str).tolist()\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    preds.extend(classify_sentiment_batch(batch))\n",
    "\n",
    "df[\"predicted_labels\"] = preds\n",
    "\n",
    "def calculate_accuracy(true_col, pred_col):\n",
    "    correct = (true_col == pred_col).sum()\n",
    "    total = len(true_col)\n",
    "    return correct / total\n",
    "\n",
    "acc = calculate_accuracy(df[\"category\"], df[\"predicted_labels\"])\n",
    "print(\"Accuracy:\", acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNO6js7K7WY2IrYliEmztmo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
